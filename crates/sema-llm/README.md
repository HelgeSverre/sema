# sema-llm

LLM provider integrations for the [Sema](https://sema-lang.com) programming language.

Provides multi-provider LLM support:

- **Providers** â€” Anthropic, OpenAI, Gemini, Groq, xAI, Mistral, Ollama, and any OpenAI-compatible endpoint
- **Auto-configuration** â€” detects API keys from environment variables
- **Dynamic pricing** â€” fetches model pricing from [llm-prices.com](https://www.llm-prices.com) with disk cache fallback
- **Budget tracking** â€” per-session cost tracking with configurable limits
- **Custom providers** â€” define providers in Sema code with `llm/define-provider`

## Usage

This is an internal crate. If you want to embed Sema in your application, use [`sema-lang`](https://crates.io/crates/sema-lang) instead:

```toml
[dependencies]
sema-lang = "1.6"
```

ðŸ“– [LLM primitives](https://sema-lang.com/docs/llm/) Â· [Documentation](https://sema-lang.com/docs/) Â· [GitHub](https://github.com/helgesverre/sema)
